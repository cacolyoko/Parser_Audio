{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as mp\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "import speech_recognition as sr\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "from google.cloud import speech\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables para utilizar siempre el mismo nombre de archivo\n",
    "\n",
    "name_file = \"Caco\"\n",
    "n_file = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraigo el audio del vídeo\n",
    "\n",
    "video = mp.VideoFileClip(\".\\\\videos\\\\{}_{}.mp4\".format(name_file, n_file))\n",
    "audio = video.audio.write_audiofile(r\".\\\\audios\\\\{}_{}.wav\".format(name_file, n_file)) # de cuando se recibia un vídeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECONOCEDOR DE GOOGLE CLOUD - SPEECH TO TEXT API\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'key.json'\n",
    "speech_client = speech.SpeechClient()\n",
    "\n",
    "audio_filename = \".\\\\audios\\\\{}_{}.wav\".format(name_file, n_file)\n",
    "with open(audio_filename, 'rb') as f:\n",
    "    byte_data_wav = f.read()\n",
    "\n",
    "audio_wav = speech.RecognitionAudio(content=byte_data_wav)\n",
    "config_wav = speech.RecognitionConfig(\n",
    "    sample_rate_hertz = 44100, # el sample ratio de hercios para los archivos WAV\n",
    "    language_code = 'es-ES', # código de idioma para español de España\n",
    "    enable_word_time_offsets = True, # tag para obtener los timestamps de las palabras\n",
    "    audio_channel_count = 2\n",
    ")\n",
    "\n",
    "# Aquí se usa el reconocedor para devolver un JSON con la transcripción generada\n",
    "# Esto es para cuando el archivo de audio dura menos de 1 minuto y pesa menos de 10Mb\n",
    "response_wav = speech_client.recognize(\n",
    "    config = config_wav,\n",
    "    audio = audio_wav\n",
    ")\n",
    "\n",
    "# Esto sirve para cuando el archivo supera el minuto y las 10Mb de tamaño\n",
    "'''media_uri = 'gs://tfg-cdr/Javi_5.wav'\n",
    "long_audio_wav = speech.RecognitionAudio(uri=media_uri)\n",
    "long_config_wav = speech.RecognitionConfig(\n",
    "    sample_rate_hertz = 44100,\n",
    "    language_code = 'es-ES',\n",
    "    enable_word_time_offsets = True,\n",
    "    audio_channel_count = 2\n",
    ")\n",
    "\n",
    "long_response_wav = speech_client.long_running_recognize(\n",
    "    config = long_config_wav,\n",
    "    audio = long_audio_wav\n",
    ")\n",
    "\n",
    "response_wav = long_response_wav.result(timeout=90)'''\n",
    "\n",
    "# Se abre un JSON nuevo y se carga en la variable \"data\"\n",
    "json_filename = \".\\\\t_raw\\\\{}_{}.json\".format(name_file, n_file)\n",
    "json_filename_rev = \".\\\\t_processed\\\\{}_{}.json\".format(name_file, n_file)\n",
    "with open(json_filename, \"w\", encoding='utf-8') as f:\n",
    "    json.dump([], f)\n",
    "with open(json_filename_rev, \"w\", encoding='utf-8') as f:\n",
    "    json.dump([], f)\n",
    "with open(json_filename, \"r\", encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Cargo la lista de stopwords para el español\n",
    "sw = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "# Navego por el JSON resultado de la transcripción\n",
    "for result in response_wav.results:\n",
    "    alternative = result.alternatives[0]\n",
    "    for word_info in alternative.words:\n",
    "        word = word_info.word\n",
    "        word = word.lower()\n",
    "        start_time = word_info.start_time.total_seconds()\n",
    "        # Elimino ya las palabras que sean stopwords excepto la palabra \"no\"\n",
    "        if word not in sw or word == 'no':\n",
    "            # Genero un pequeño diccionario a modo de objeto para guardar la información de la palabra y su start time\n",
    "            entry = {\n",
    "                'word': word,\n",
    "                'start': start_time\n",
    "            }\n",
    "            # Lo meto en la lista \"data\"\n",
    "            data.append(entry)\n",
    "\n",
    "# Guardo el contenido de \"data\" en el archivo JSON para revisarlo\n",
    "with open(json_filename, \"w\", encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False)\n",
    "with open(json_filename_rev, \"w\", encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False)\n",
    "\n",
    "print(\"Se han generado correctamente los siguientes archivos:\\n{}\\n{}\".format(json_filename, json_filename_rev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estemización de las palabras transcritas desde la API de GOOGLE CLOUD SPEECH TO TEXT\n",
    "\n",
    "json_filename = \".\\\\t_processed\\\\{}_{}.json\".format(name_file, n_file)\n",
    "with open(json_filename, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "json_filename_rev = \".\\\\t_processed\\\\{}_{}_rev.json\".format(name_file, n_file)\n",
    "with open(json_filename_rev, \"w\", encoding='utf-8') as f:\n",
    "    data_rev = json.dump([], f)\n",
    "with open(json_filename_rev, \"r\", encoding='utf-8') as f:\n",
    "    data_rev = json.load(f)\n",
    "\n",
    "for t in data:\n",
    "    word = t['word']\n",
    "    start = t['start']\n",
    "    entry = {\n",
    "        'word': word,\n",
    "        'stem': SnowballStemmer('spanish').stem(word),\n",
    "        'start': start\n",
    "    }\n",
    "    data_rev.append(entry)\n",
    "\n",
    "with open(json_filename_rev, \"w\", encoding='utf-8') as f:\n",
    "    json.dump(data_rev, f, ensure_ascii=False)\n",
    "\n",
    "print(\"Se ha procesado por medio de stemming el archivo {}_{}.json y se ha generado el siguiente archivo:\\n{}\".format(name_file, n_file, json_filename_rev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añaden los tags y se genera el JSON de PVA completo\n",
    "\n",
    "instrucciones = {\n",
    "    'aceler': 'T-ON',\n",
    "    'acerc rotond': 'APROX',\n",
    "    'rotond medi': 'RND-MD',\n",
    "    'manteng aceler': 'T-HOLD',\n",
    "    'rotond cerc': 'RND-NR',\n",
    "    'izquierd libr': 'L-FREE',\n",
    "    'izquierd ocup': 'L-BUSY',\n",
    "    'fren ced': 'B-ON',\n",
    "    'fren stop': 'B-ON',\n",
    "    'fren': 'B-ON',\n",
    "    'stop': 'B-ON',\n",
    "    'ced': 'RND-IN',\n",
    "    'entro rotond': 'RND-IN',\n",
    "    'gir derech': 'TURN-R',\n",
    "    'intermitent izquierd': 'LB-ON',\n",
    "    'pong intermitent izquierd': 'LB-ON',\n",
    "    'intermitent izquierd rotond': 'LB-ON',\n",
    "    'rotond gir izquierd': 'TURN-L',\n",
    "    'rotond gir derech': 'TURN-R',\n",
    "    'gir izquierd': 'TURN-L',\n",
    "    'quit intermitent': 'BLK-OFF',\n",
    "    'intermitent derech': 'RB-ON',\n",
    "    'pong intermitent derech': 'RB-ON',\n",
    "    'intermitent derech rotond': 'RB-ON',\n",
    "    'salg rotond': 'RND-EXIT',\n",
    "    'aproxim rotond': 'APROX',\n",
    "    'levant aceler': 'T-OFF',\n",
    "    'levant pie': 'T-OFF',\n",
    "    'levant pie aceler': 'T-OFF',\n",
    "    'no vien nadi': 'RND-CLEAR',\n",
    "    'gir izquierd rotond': 'TURN-L',\n",
    "    'gir derech rotond': 'TURN-R',\n",
    "    'atencion ciclist': 'CAR-NR',\n",
    "    'atencion coch': 'CAR-NR',\n",
    "    'centr libr': 'F-FREE',\n",
    "    'centr ocup': 'F-BUSY',\n",
    "    'enderez': 'TURN-STR',\n",
    "    'sig rect': 'STR',\n",
    "    'rect': 'STR',\n",
    "    'primer sal': 'EXIT-1',\n",
    "    'segund sal': 'EXIT-2',\n",
    "    'tercer sal': 'EXIT-3',\n",
    "    'cuart sal': 'EXIT-4'\n",
    "}\n",
    "\n",
    "# Utilizar este json_filename si se ha transcrito con el reconocedor básico\n",
    "#json_filename = \".\\\\t_processed\\\\{}_{}.json\".format(name_file, n_file)\n",
    "\n",
    "# Utilizar este json_filename si se ha transcrito con el reconocedor de la API de GOOGLE CLOUD SPEECH TO TEXT\n",
    "json_filename = \".\\\\t_processed\\\\{}_{}_rev.json\".format(name_file, n_file)\n",
    "\n",
    "with open(json_filename, \"r\", encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "dataset = {}\n",
    "count = 0\n",
    "\n",
    "for i, x in enumerate(data):\n",
    "    w = x['stem']\n",
    "    if (len(data) > i + 2) and x['stem'] + ' ' + data[i + 1]['stem'] + ' ' + data[i + 2]['stem'] in instrucciones:\n",
    "        sentence = x['word'] + ' ' + data[i + 1]['word'] + ' ' + data[i + 2]['word']\n",
    "        sentence_t = x['stem'] + ' ' + data[i + 1]['stem'] + ' ' + data[i + 2]['stem']\n",
    "        dataset[\"inst_{}\".format(count)] = [x, data[i + 1], data[i + 2]]\n",
    "        dataset[\"inst_{}\".format(count)].append({\"tag\": instrucciones[sentence_t], \"sentence\": sentence})\n",
    "        count += 1\n",
    "        del data[i + 1]\n",
    "        del data[i + 1]\n",
    "    elif (len(data) > i + 1) and x['stem'] + ' ' + data[i + 1]['stem'] in instrucciones:\n",
    "        sentence = x['word'] + ' ' + data[i + 1]['word']\n",
    "        sentence_t = x['stem'] + ' ' + data[i + 1]['stem']\n",
    "        dataset[\"inst_{}\".format(count)] = [x, data[i + 1]]\n",
    "        dataset[\"inst_{}\".format(count)].append({\"tag\": instrucciones[sentence_t], \"sentence\": sentence})\n",
    "        count += 1\n",
    "        del data[i + 1]\n",
    "    elif w in instrucciones:\n",
    "        dataset[\"inst_{}\".format(count)] = [x]\n",
    "        dataset[\"inst_{}\".format(count)].append({\"tag\": instrucciones[w], \"sentence\": x['word']})\n",
    "        count += 1\n",
    "\n",
    "json_out = \".\\\\t_tags\\\\{}_{}.json\".format(name_file, n_file)\n",
    "with open(json_out, \"w\", encoding='utf-8') as f:\n",
    "    json.dump(dataset, f, ensure_ascii=False)\n",
    "\n",
    "print(\"Se ha etiquetado correctamente el archivo y se ha generado el siguiente fichero:\\n{}\".format(json_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusión del JSON de pensamiento en voz alta con el JSON de Carla correspondiente a cada grabación\n",
    "\n",
    "pva_json_filename = \".\\\\t_tags\\\\{}_{}.json\".format(name_file, n_file)\n",
    "with open(pva_json_filename, 'r', encoding='utf-8') as f:\n",
    "    pva_data = json.load(f)\n",
    "\n",
    "carla_json_filename = \".\\\\dataset\\\\parse\\\\{}_{}\\\\SIM_DATA.json\".format(name_file, n_file)\n",
    "with open(carla_json_filename, 'r', encoding='utf-8') as f:\n",
    "    carla_data = json.load(f)\n",
    "\n",
    "for inst in pva_data:\n",
    "    start = pva_data[inst][0]['start']\n",
    "    last = pva_data[inst][-1]\n",
    "    for i, tick in enumerate(carla_data):\n",
    "        if (i + 1) < len(carla_data):\n",
    "            tick_next = carla_data[i + 1]\n",
    "            if tick['timestamp'] <= start and tick_next['timestamp'] > start:\n",
    "                carla_dic = dict(tick)\n",
    "                last['carla'] = carla_dic\n",
    "                light_state = last['carla']['control']['light_state']\n",
    "                light_state = \"Left_Blinker\" if light_state == 32 else \"Right_Blinker\" if light_state == 16 else \"NONE\"\n",
    "                last['carla']['control']['light_state'] = light_state\n",
    "                del last['carla']['timestamp']\n",
    "                break\n",
    "            \n",
    "final_json_filename = \".\\\\t_merged\\\\{}_{}.json\".format(name_file, n_file)\n",
    "with open(final_json_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(pva_data, f, ensure_ascii=False)\n",
    "\n",
    "print(\"Se han fusionado correctamente los datos de verbalización con lo datos de Carla en el siguiente archivo:\\n{}\".format(final_json_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mete la información de todos los JSON para la sección de APROXIMACIÓN A ROTONDA en un libro de Excel. Cada JSON está en una hoja distinta\n",
    "\n",
    "dir_path = '.\\\\t_merged'\n",
    "file_list = os.listdir(dir_path)\n",
    "\n",
    "for file in file_list:\n",
    "    json_path = \".\\\\t_merged\\\\{}\".format(file)\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    ceda_md = 0\n",
    "    ceda_nr = 0\n",
    "    ceda_in = 0\n",
    "\n",
    "    zi = 0\n",
    "    zc = 0\n",
    "\n",
    "    salida = 3\n",
    "\n",
    "    for d in data:\n",
    "        inst = data[d][-1]\n",
    "        if inst['sentence'] == 'primera salida':\n",
    "            salida = 1\n",
    "        elif inst['sentence'] == 'segunda salida':\n",
    "            salida = 2\n",
    "        elif inst['sentence'] == 'tercera salida':\n",
    "            salida = 3\n",
    "        elif inst['sentence'] == 'cuarta salida':\n",
    "            salida = 4\n",
    "\n",
    "    index_list = []\n",
    "    data_list = []\n",
    "\n",
    "    for d in data:\n",
    "        inst = data[d][-1]\n",
    "        carla = inst['carla']\n",
    "        control = inst['carla']['control']\n",
    "        loc = inst['carla']['location']\n",
    "        dist = round(math.sqrt((-6.22 - loc['x'])**2 + (-28.37 - loc['y'])**2), 2)\n",
    "\n",
    "        if inst['sentence'] == 'primera salida':\n",
    "            continue\n",
    "        elif inst['sentence'] == 'segunda salida':\n",
    "            continue\n",
    "        elif inst['sentence'] == 'tercera salida':\n",
    "            continue\n",
    "        elif inst['sentence'] == 'cuarta salida':\n",
    "            continue\n",
    "\n",
    "        if inst['sentence'] == 'izquierda ocupada':\n",
    "            zi = 1\n",
    "        elif inst['sentence'] == 'centro ocupado':\n",
    "            zc = 1\n",
    "        elif inst['sentence'] == 'izquierda libre' or inst['sentence'] == 'centro libre':\n",
    "            zi = 0\n",
    "            zc = 0\n",
    "\n",
    "        if inst['sentence'] == 'rotonda media':\n",
    "            ceda_md = 1\n",
    "            continue\n",
    "        elif inst['sentence'] == 'rotonda cerca':\n",
    "            ceda_nr = 1\n",
    "            ceda_md = 0\n",
    "            continue\n",
    "        elif inst['sentence'] == 'izquierda libre' or inst['sentence'] == 'izquierda ocupada':\n",
    "            ceda_in = 1\n",
    "            ceda_nr = 0\n",
    "        elif inst['sentence'] == 'entro rotonda':\n",
    "            ceda_in = 0\n",
    "            entry = {'instruction': inst['sentence'], 'tag': inst['tag'], 'salida': salida, 'ceda_md': ceda_md, 'ceda_nr': ceda_nr, 'ceda_in': ceda_in, 'zi_ocupada': zi, 'zc_ocupada': zc, 'metros_ceda': dist, 'throttle': control['throttle'], 'steer': control['steer'], 'brake': control['brake'], 'light_state': control['light_state'], 'hand_brake': control['hand_brake'], 'reverse': control['reverse'], 'speed': carla['speed (km/h)'], 'loc_x': loc['x'], 'loc_y': loc['y'], 'loc_z': loc['z']}\n",
    "            index_list.append(d)\n",
    "            data_list.append(entry)\n",
    "            break\n",
    "        \n",
    "        entry = {'instruction': inst['sentence'], 'tag': inst['tag'], 'salida': salida, 'ceda_md': ceda_md, 'ceda_nr': ceda_nr, 'ceda_in': ceda_in, 'zi_ocupada': zi, 'zc_ocupada': zc, 'metros_ceda': dist, 'throttle': control['throttle'], 'steer': control['steer'], 'brake': control['brake'], 'light_state': control['light_state'], 'hand_brake': control['hand_brake'], 'reverse': control['reverse'], 'speed': carla['speed (km/h)'], 'loc_x': loc['x'], 'loc_y': loc['y'], 'loc_z': loc['z']}\n",
    "        index_list.append(d)\n",
    "        data_list.append(entry)\n",
    "\n",
    "    df = pd.DataFrame(data=data_list, index=index_list)\n",
    "    excel_path_train = \".\\\\dataset_excel\\\\aprox_rotonda_train.xlsx\"\n",
    "    excel_path_test = \".\\\\dataset_excel\\\\aprox_rotonda_test.xlsx\"\n",
    "    sheet_name = 'Aprox_Rotonda'\n",
    "    sheet_name_test = re.sub('.json', '', file)\n",
    "    try:\n",
    "        with pd.ExcelWriter(excel_path_train, engine='openpyxl', mode='a') as writer:\n",
    "            writer.book = load_workbook(excel_path_train)\n",
    "            writer.sheets = {ws.title: ws for ws in writer.book.worksheets}\n",
    "            startrow = writer.sheets[sheet_name].max_row\n",
    "            df.to_excel(writer, sheet_name, header=False, startrow=startrow, index=False)\n",
    "    except FileNotFoundError:\n",
    "        with pd.ExcelWriter(excel_path_train, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name, index=False)\n",
    "\n",
    "    try:\n",
    "        with pd.ExcelWriter(excel_path_test, engine='openpyxl', mode='a') as writer:\n",
    "            df.to_excel(writer, sheet_name_test, index=False)\n",
    "    except FileNotFoundError:\n",
    "        with pd.ExcelWriter(excel_path_test, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name_test, index=False)\n",
    "\n",
    "print(\"Se han generado correctamente los archivos aprox_rotonda_train.xlsx y aprox_rotonda_test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mete la información de todos los JSON para la sección de DENTRO DE LA ROTONDA en un libro de Excel. Cada JSON está en una hoja distinta\n",
    "\n",
    "dir_path = '.\\\\t_merged'\n",
    "file_list = os.listdir(dir_path)\n",
    "\n",
    "for file in file_list:\n",
    "    json_path = \".\\\\t_merged\\\\{}\".format(file)\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    index_list = []\n",
    "    data_list = []\n",
    "    get_data = False\n",
    "    car_nr = 0\n",
    "    salida = 3\n",
    "\n",
    "    for d in data:\n",
    "        inst = data[d][-1]\n",
    "        if inst['sentence'] == 'primera salida':\n",
    "            salida = 1\n",
    "        elif inst['sentence'] == 'segunda salida':\n",
    "            salida = 2\n",
    "        elif inst['sentence'] == 'tercera salida':\n",
    "            salida = 3\n",
    "        elif inst['sentence'] == 'cuarta salida':\n",
    "            salida = 4\n",
    "\n",
    "    for d in data:\n",
    "        inst = data[d][-1]\n",
    "        carla = inst['carla']\n",
    "        control = inst['carla']['control']\n",
    "        loc = inst['carla']['location']\n",
    "        if get_data:\n",
    "            if inst['sentence'] == 'primera salida':\n",
    "                continue\n",
    "            elif inst['sentence'] == 'segunda salida':\n",
    "                continue\n",
    "            elif inst['sentence'] == 'tercera salida':\n",
    "                continue\n",
    "            elif inst['sentence'] == 'cuarta salida':\n",
    "                continue\n",
    "\n",
    "            if inst['tag'] == 'CAR-NR':\n",
    "                car_nr = 1\n",
    "            \n",
    "            entry = {'instruction': inst['sentence'], 'tag': inst['tag'], 'salida': salida, 'coche_cerca': car_nr, 'throttle': control['throttle'], 'steer': control['steer'], 'brake': control['brake'], 'light_state': control['light_state'], 'hand_brake': control['hand_brake'], 'reverse': control['reverse'], 'speed': carla['speed (km/h)'], 'loc_x': loc['x'], 'loc_y': loc['y'], 'loc_z': loc['z']}\n",
    "            index_list.append(d)\n",
    "            data_list.append(entry)\n",
    "            \n",
    "        if inst['sentence'] == \"entro rotonda\":\n",
    "            get_data = True\n",
    "        elif inst['sentence'] == 'salgo rotonda':\n",
    "            get_data = False\n",
    "\n",
    "        if inst['sentence'] == 'freno':\n",
    "            car_nr = 0\n",
    "        \n",
    "    df = pd.DataFrame(data=data_list, index=index_list)\n",
    "    excel_path_train = \".\\\\dataset_excel\\\\dentro_rotonda_train.xlsx\"\n",
    "    excel_path_test = \".\\\\dataset_excel\\\\dentro_rotonda_test.xlsx\"\n",
    "    sheet_name = 'Dentro_Rotonda'\n",
    "    sheet_name_test = re.sub('.json', '', file)\n",
    "    try:\n",
    "        with pd.ExcelWriter(excel_path_train, engine='openpyxl', mode='a') as writer:\n",
    "            writer.book = load_workbook(excel_path_train)\n",
    "            writer.sheets = {ws.title: ws for ws in writer.book.worksheets}\n",
    "            startrow = writer.sheets[sheet_name].max_row\n",
    "            df.to_excel(writer, sheet_name, header=False, startrow=startrow, index=False)\n",
    "    except FileNotFoundError:\n",
    "        with pd.ExcelWriter(excel_path_train, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name, index=False)\n",
    "\n",
    "    try:\n",
    "        with pd.ExcelWriter(excel_path_test, engine='openpyxl', mode='a') as writer:\n",
    "            df.to_excel(writer, sheet_name_test, index=False)\n",
    "    except FileNotFoundError:\n",
    "        with pd.ExcelWriter(excel_path_test, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name_test, index=False)\n",
    "\n",
    "print(\"Se han generado correctamente los archivos dentro_rotonda_train.xlsx y dentro_rotonda_test.xlsx\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mete la información de todos los JSON para la sección de SALIDA DE LA ROTONDA en un libro de Excel. Cada JSON está en una hoja distinta\n",
    "\n",
    "dir_path = '.\\\\t_merged'\n",
    "file_list = os.listdir(dir_path)\n",
    "\n",
    "for file in file_list:\n",
    "    json_path = \".\\\\t_merged\\\\{}\".format(file)\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    index_list = []\n",
    "    data_list = []\n",
    "    get_data = False\n",
    "    car_nr = 0\n",
    "    salida = 3\n",
    "\n",
    "    for d in data:\n",
    "        inst = data[d][-1]\n",
    "        if inst['sentence'] == 'primera salida':\n",
    "            salida = 1\n",
    "        elif inst['sentence'] == 'segunda salida':\n",
    "            salida = 2\n",
    "        elif inst['sentence'] == 'tercera salida':\n",
    "            salida = 3\n",
    "        elif inst['sentence'] == 'cuarta salida':\n",
    "            salida = 4\n",
    "\n",
    "    for d in data:\n",
    "        inst = data[d][-1]\n",
    "        carla = inst['carla']\n",
    "        control = inst['carla']['control']\n",
    "        loc = inst['carla']['location']\n",
    "        if get_data:\n",
    "            if inst['sentence'] == 'primera salida':\n",
    "                continue\n",
    "            elif inst['sentence'] == 'segunda salida':\n",
    "                continue\n",
    "            elif inst['sentence'] == 'tercera salida':\n",
    "                continue\n",
    "            elif inst['sentence'] == 'cuarta salida':\n",
    "                continue\n",
    "\n",
    "            if inst['tag'] == 'CAR-NR':\n",
    "                car_nr = 1\n",
    "\n",
    "            entry = {'instruction': inst['sentence'], 'tag': inst['tag'], 'salida': salida, 'coche_cerca': car_nr, 'throttle': control['throttle'], 'steer': control['steer'], 'brake': control['brake'], 'light_state': control['light_state'], 'hand_brake': control['hand_brake'], 'reverse': control['reverse'], 'speed': carla['speed (km/h)'], 'loc_x': loc['x'], 'loc_y': loc['y'], 'loc_z': loc['z']}\n",
    "            index_list.append(d)\n",
    "            data_list.append(entry)\n",
    "\n",
    "        if inst['sentence'] == 'freno':\n",
    "            car_nr = 0\n",
    "        if inst['sentence'] == \"salgo rotonda\":\n",
    "            get_data = True\n",
    "        \n",
    "    df = pd.DataFrame(data=data_list, index=index_list)\n",
    "    excel_path_train = \".\\\\dataset_excel\\\\salida_rotonda_train.xlsx\"\n",
    "    excel_path_test = \".\\\\dataset_excel\\\\salida_rotonda_test.xlsx\"\n",
    "    sheet_name = 'Salida_Rotonda'\n",
    "    sheet_name_test = re.sub('.json', '', file)\n",
    "    try:\n",
    "        with pd.ExcelWriter(excel_path_train, engine='openpyxl', mode='a') as writer:\n",
    "            writer.book = load_workbook(excel_path_train)\n",
    "            writer.sheets = {ws.title: ws for ws in writer.book.worksheets}\n",
    "            startrow = writer.sheets[sheet_name].max_row\n",
    "            df.to_excel(writer, sheet_name, header=False, startrow=startrow, index=False)\n",
    "    except FileNotFoundError:\n",
    "        with pd.ExcelWriter(excel_path_train, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name, index=False)\n",
    "\n",
    "    try:\n",
    "        with pd.ExcelWriter(excel_path_test, engine='openpyxl', mode='a') as writer:\n",
    "            df.to_excel(writer, sheet_name_test, index=False)\n",
    "    except FileNotFoundError:\n",
    "        with pd.ExcelWriter(excel_path_test, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name_test, index=False)\n",
    "\n",
    "print(\"Se han generado correctamente los archivos salida_rotonda_train.xlsx y salida_rotonda_test.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bbc1c972b9b102a93c73821bc38c3abe2659eb37faffcc2a906a1a9997853cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
