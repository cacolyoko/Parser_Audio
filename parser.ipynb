{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as mp\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "import speech_recognition as sr\n",
    "from tkinter.filedialog import askopenfilename\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_file = \"Javi\"\n",
    "n_file = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:   0%|          | 0/1365 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in .\\\\audios\\\\Javi_5.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Extraigo el audio del vídeo\n",
    "\n",
    "video = mp.VideoFileClip(\".\\\\videos\\\\{}_{}.mp4\".format(name_file, n_file))\n",
    "audio = video.audio.write_audiofile(r\".\\\\audios\\\\{}_{}.wav\".format(name_file, n_file)) # de cuando se recibia un vídeo\n",
    "# HASTA AQUÍ SI SE UTILIZA EL GOOGLE CLOUD SPEECH TO TEXT API\n",
    "recibido_cambio = sr.Recognizer()\n",
    "speech_audio = sr.AudioFile(\".\\\\audios\\\\{}_{}.wav\".format(name_file, n_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrículas celador aproximó rotonda rotonda media levanto el pie del acelerador rotonda cerca izquierda libre intermitente izquierda entre la rotonda gira izquierda acelero quita el intermitente intermitente derecha giro derecha salgo de rotonda sigue recto\n"
     ]
    }
   ],
   "source": [
    "# Reconocedor básico (ANTIGUO)\n",
    "\n",
    "with speech_audio as fuente:\n",
    "    #Se reduce el ruido\n",
    "    recibido_cambio.adjust_for_ambient_noise(fuente)\n",
    "    audio = recibido_cambio.record(fuente)\n",
    "    #Transcripcion\n",
    "    texto_transcrito = recibido_cambio.recognize_google(audio, language = \"es-ES\")\n",
    "    print(texto_transcrito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECONOCEDOR DE GOOGLE CLOUD - SPEECH TO TEXT API\n",
    "\n",
    "import os\n",
    "from google.cloud import speech\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'key.json'\n",
    "speech_client = speech.SpeechClient()\n",
    "\n",
    "audio_filename = \".\\\\audios\\\\{}_{}.wav\".format(name_file, n_file)\n",
    "with open(audio_filename, 'rb') as f:\n",
    "    byte_data_wav = f.read()\n",
    "\n",
    "audio_wav = speech.RecognitionAudio(content=byte_data_wav)\n",
    "config_wav = speech.RecognitionConfig(\n",
    "    sample_rate_hertz = 44100, # el sample ratio de hercios para los archivos WAV\n",
    "    language_code = 'es-ES', # código de idioma para español de España\n",
    "    enable_word_time_offsets = True, # tag para obtener los timestamps de las palabras\n",
    "    audio_channel_count = 2\n",
    ")\n",
    "\n",
    "# Aquí se usa el reconocedor para devolver un JSON con la transcripción generada\n",
    "# Esto es para cuando el archivo de audio dura menos de 1 minuto y pesa menos de 10Mb\n",
    "response_wav = speech_client.recognize(\n",
    "    config = config_wav,\n",
    "    audio = audio_wav\n",
    ")\n",
    "\n",
    "# Esto sirve para cuando el archivo supera el minuto y las 10Mb de tamaño\n",
    "'''media_uri = 'gs://tfg-cdr/Javi_5.wav'\n",
    "long_audio_wav = speech.RecognitionAudio(uri=media_uri)\n",
    "long_config_wav = speech.RecognitionConfig(\n",
    "    sample_rate_hertz = 44100,\n",
    "    language_code = 'es-ES',\n",
    "    enable_word_time_offsets = True,\n",
    "    audio_channel_count = 2\n",
    ")\n",
    "\n",
    "long_response_wav = speech_client.long_running_recognize(\n",
    "    config = long_config_wav,\n",
    "    audio = long_audio_wav\n",
    ")\n",
    "\n",
    "response_wav = long_response_wav.result(timeout=90)'''\n",
    "\n",
    "# Se abre un JSON nuevo y se carga en la variable \"data\"\n",
    "json_filename = \".\\\\t_raw\\\\{}_{}.json\".format(name_file, n_file)\n",
    "with open(json_filename, \"w\", encoding='utf-8') as f:\n",
    "    data = json.dump([], f)\n",
    "with open(json_filename, \"r\", encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Cargo la lista de stopwords para el español\n",
    "sw = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "# Navego por el JSON resultado de la transcripción\n",
    "for result in response_wav.results:\n",
    "    alternative = result.alternatives[0]\n",
    "    for word_info in alternative.words:\n",
    "        word = word_info.word\n",
    "        word = word.lower()\n",
    "        start_time = word_info.start_time.total_seconds()\n",
    "        # Elimino ya las palabras que sean stopwords excepto la palabra \"no\"\n",
    "        if word not in sw or word == 'no':\n",
    "            # Genero un pequeño diccionario a modo de objeto para guardar la información de la palabra y su start time\n",
    "            entry = {\n",
    "                'word': word,\n",
    "                'start': start_time\n",
    "            }\n",
    "            # Lo meto en la lista \"data\"\n",
    "            data.append(entry)\n",
    "\n",
    "# Guardo el contenido de \"data\" en el archivo JSON para revisarlo\n",
    "with open(json_filename, \"w\", encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estemización de las palabras transcritas desde la API de GOOGLE CLOUD SPEECH TO TEXT\n",
    "\n",
    "json_filename = \".\\\\t_processed\\\\{}_{}.json\".format(name_file, n_file)\n",
    "with open(json_filename, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "json_filename_rev = \".\\\\t_processed\\\\{}_{}_rev.json\".format(name_file, n_file)\n",
    "with open(json_filename_rev, \"w\", encoding='utf-8') as f:\n",
    "    data_rev = json.dump([], f)\n",
    "with open(json_filename_rev, \"r\", encoding='utf-8') as f:\n",
    "    data_rev = json.load(f)\n",
    "\n",
    "for t in data:\n",
    "    word = t['word']\n",
    "    start = t['start']\n",
    "    entry = {\n",
    "        'word': word,\n",
    "        'stem': SnowballStemmer('spanish').stem(word),\n",
    "        'start': start\n",
    "    }\n",
    "    data_rev.append(entry)\n",
    "\n",
    "with open(json_filename_rev, \"w\", encoding='utf-8') as f:\n",
    "    json.dump(data_rev, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha guardado la transcripción en el archivo .\\t_raw\\Borja_7.txt\n"
     ]
    }
   ],
   "source": [
    "# Guardo el texto en bruto transcrito de los audios (ESTO ES CON EL RECONOCEDOR ANTIGUO)\n",
    "\n",
    "filename_t_raw = \".\\\\t_raw\\\\{}_{}.txt\".format(name_file, n_file)\n",
    "with open(filename_t_raw, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(texto_transcrito)\n",
    "print(\"Se ha guardado la transcripción en el archivo \" + filename_t_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARSEO PARA LOS DATOS DEL RECONOCEDOR BÁSICO\n",
    "\n",
    "f = open ('.\\\\t_processed\\\\{}_{}.txt'.format(name_file, n_file),'r',encoding='utf8')\n",
    "text = f.read()\n",
    "\n",
    "sw = set(stopwords.words(\"spanish\"))\n",
    "# Limpio con regex el texto\n",
    "text = re.sub('[%s]' % re.escape(string.punctuation + \"'\" + '\"' + \"’\" + '”' + '“' + \"•‘\"), ' ', str(text))\n",
    "text = re.sub('\\w*\\d\\w*', ' ', str(text))\n",
    "# Pongo el texto en minúsculas\n",
    "text = text.lower()\n",
    "# Tokenizo por palabras\n",
    "text = word_tokenize(str(text), language='spanish')\n",
    "tokens = []\n",
    "for t in text:\n",
    "    if not t in sw or t == 'no':\n",
    "        tokens.append(t)\n",
    "\n",
    "json_filename = \".\\\\t_processed\\\\{}_{}.json\".format(name_file, n_file)\n",
    "with open(json_filename, \"w\", encoding='utf-8') as f:\n",
    "    data = json.dump([], f)\n",
    "with open(json_filename, \"r\", encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "for t in tokens:\n",
    "    entry = {\n",
    "        'word': t,\n",
    "        'stem': SnowballStemmer('spanish').stem(t),\n",
    "        'start': 0.0\n",
    "    }\n",
    "    data.append(entry)\n",
    "\n",
    "with open(json_filename, \"w\", encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añaden los tags y se genera el JSON de PVA completo\n",
    "\n",
    "instrucciones = {\n",
    "    'aceler': 'T-ON',\n",
    "    'acerc rotond': 'APROX',\n",
    "    'rotond medi': 'RND-MD',\n",
    "    'manteng aceler': 'T-HOLD',\n",
    "    'rotond cerc': 'RND-NR',\n",
    "    'izquierd libr': 'L-FREE',\n",
    "    'izquierd ocup': 'L-BUSY',\n",
    "    'fren ced': 'B-ON',\n",
    "    'fren stop': 'B-ON',\n",
    "    'fren': 'B-ON',\n",
    "    'stop': 'B-ON',\n",
    "    'ced': 'RND-IN',\n",
    "    'entro rotond': 'RND-IN',\n",
    "    'gir derech': 'TURN-R',\n",
    "    'intermitent izquierd': 'LB-ON',\n",
    "    'pong intermitent izquierd': 'LB-ON',\n",
    "    'intermitent izquierd rotond': 'LB-ON',\n",
    "    'rotond gir izquierd': 'TURN-L',\n",
    "    'rotond gir derech': 'TURN-R',\n",
    "    'gir izquierd': 'TURN-L',\n",
    "    'quit intermitent': 'BLK-OFF',\n",
    "    'intermitent derech': 'RB-ON',\n",
    "    'pong intermitent derech': 'RB-ON',\n",
    "    'intermitent derech rotond': 'RB-ON',\n",
    "    'salg rotond': 'RND-EXIT',\n",
    "    'aproxim rotond': 'APROX',\n",
    "    'levant aceler': 'T-OFF',\n",
    "    'levant pie': 'T-OFF',\n",
    "    'levant pie aceler': 'T-OFF',\n",
    "    'no vien nadi': 'RND-CLEAR',\n",
    "    'gir izquierd rotond': 'TURN-L',\n",
    "    'gir derech rotond': 'TURN-R',\n",
    "    'atencion ciclist': 'CAR-NR',\n",
    "    'atencion coche': 'CAR-NR',\n",
    "    'centr libr': 'L-FREE',\n",
    "    'centr ocup': 'F-BUSY',\n",
    "    'enderez': 'TURN-STR',\n",
    "    'sig rect': 'STR',\n",
    "    'rect': 'STR'\n",
    "}\n",
    "\n",
    "# Utilizar este json_filename si se ha transcrito con el reconocedor básico\n",
    "#json_filename = \".\\\\t_processed\\\\{}_{}.json\".format(name_file, n_file)\n",
    "\n",
    "# Utilizar este json_filename si se ha transcrito con el reconocedor de la API de GOOGLE CLOUD SPEECH TO TEXT\n",
    "json_filename = \".\\\\t_processed\\\\{}_{}_rev.json\".format(name_file, n_file)\n",
    "\n",
    "with open(json_filename, \"r\", encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "dataset = {}\n",
    "count = 0\n",
    "\n",
    "for i, x in enumerate(data):\n",
    "    w = x['stem']\n",
    "    if (len(data) > i + 2) and x['stem'] + ' ' + data[i + 1]['stem'] + ' ' + data[i + 2]['stem'] in instrucciones:\n",
    "        sentence = x['word'] + ' ' + data[i + 1]['word'] + ' ' + data[i + 2]['word']\n",
    "        sentence_t = x['stem'] + ' ' + data[i + 1]['stem'] + ' ' + data[i + 2]['stem']\n",
    "        dataset[\"inst_{}\".format(count)] = [x, data[i + 1], data[i + 2]]\n",
    "        dataset[\"inst_{}\".format(count)].append({\"tag\": instrucciones[sentence_t], \"sentence\": sentence})\n",
    "        count += 1\n",
    "        del data[i + 1]\n",
    "        del data[i + 1]\n",
    "    elif (len(data) > i + 1) and x['stem'] + ' ' + data[i + 1]['stem'] in instrucciones:\n",
    "        sentence = x['word'] + ' ' + data[i + 1]['word']\n",
    "        sentence_t = x['stem'] + ' ' + data[i + 1]['stem']\n",
    "        dataset[\"inst_{}\".format(count)] = [x, data[i + 1]]\n",
    "        dataset[\"inst_{}\".format(count)].append({\"tag\": instrucciones[sentence_t], \"sentence\": sentence})\n",
    "        count += 1\n",
    "        del data[i + 1]\n",
    "    elif w in instrucciones:\n",
    "        dataset[\"inst_{}\".format(count)] = [x]\n",
    "        dataset[\"inst_{}\".format(count)].append({\"tag\": instrucciones[w], \"sentence\": x['word']})\n",
    "        count += 1\n",
    "\n",
    "json_out = \".\\\\t_tags\\\\{}_{}.json\".format(name_file, n_file)\n",
    "with open(json_out, \"w\", encoding='utf-8') as f:\n",
    "    json.dump(dataset, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusión del JSON de pensamiento en voz alta con el JSON de Carla correspondiente a cada grabación\n",
    "\n",
    "pva_json_filename = \".\\\\t_tags\\\\{}_{}.json\".format(name_file, n_file)\n",
    "with open(pva_json_filename, 'r', encoding='utf-8') as f:\n",
    "    pva_data = json.load(f)\n",
    "\n",
    "carla_json_filename = \".\\\\dataset\\\\parse\\\\{}_{}\\\\SIM_DATA.json\".format(name_file, n_file)\n",
    "with open(carla_json_filename, 'r', encoding='utf-8') as f:\n",
    "    carla_data = json.load(f)\n",
    "\n",
    "for inst in pva_data:\n",
    "    start = pva_data[inst][0]['start']\n",
    "    last = pva_data[inst][-1]\n",
    "    for i, tick in enumerate(carla_data):\n",
    "        if (i + 1) < len(carla_data):\n",
    "            tick_next = carla_data[i + 1]\n",
    "            if tick['timestamp'] <= start and tick_next['timestamp'] > start:\n",
    "                carla_dic = dict(tick)\n",
    "                last['carla'] = carla_dic\n",
    "                light_state = last['carla']['control']['light_state']\n",
    "                light_state = \"Left_Blinker\" if light_state == 32 else \"Right_Blinker\" if light_state == 16 else \"NONE\"\n",
    "                last['carla']['control']['light_state'] = light_state\n",
    "                del last['carla']['timestamp']\n",
    "                break\n",
    "            \n",
    "final_json_filename = \".\\\\t_merged\\\\{}_{}.json\".format(name_file, n_file)\n",
    "with open(final_json_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(pva_data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX START TIMESTAMPS (30 divisiones a 100 divisiones)\n",
    "\n",
    "json_filename = \".\\\\t_tags\\\\Borja_7.json\"\n",
    "with open(json_filename, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for inst in data:\n",
    "    for a in data[inst]:\n",
    "        if 'start' in a:\n",
    "            start = a['start']\n",
    "            decimal_part = start % 1\n",
    "            start -= decimal_part\n",
    "            decimal_part = decimal_part * 100 / 30\n",
    "            start += decimal_part\n",
    "            a['start'] = round(start, 2)\n",
    "\n",
    "with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bbc1c972b9b102a93c73821bc38c3abe2659eb37faffcc2a906a1a9997853cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
